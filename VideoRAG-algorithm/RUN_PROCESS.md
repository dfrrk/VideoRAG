# VideoRAG-algorithm è¿è¡Œæµç¨‹åˆ†æ

æœ¬æ–‡æ¡£æ—¨åœ¨è¯¦ç»†è§£æ `videorag_longervideos.py` è„šæœ¬åœ¨ Linux (æˆ– WSL) ç¯å¢ƒä¸‹çš„å®Œæ•´æ‰§è¡Œæµç¨‹ã€‚

## è„šæœ¬æ¦‚è¿°

è¯¥è„šæœ¬çš„æ ¸å¿ƒç›®æ ‡åˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼š

1.  **å­¦ä¹ /ç´¢å¼• (Learn Phase)**: é¦–å…ˆï¼Œå®ƒä¼šå¤„ç†æŒ‡å®šæ–‡ä»¶å¤¹ä¸‹çš„æ‰€æœ‰è§†é¢‘æ–‡ä»¶ï¼Œé€šè¿‡ä¸€ç³»åˆ—å¤æ‚çš„ AI æ¨¡å‹æå–ä¿¡æ¯ï¼Œå¹¶å°†è¿™äº›ä¿¡æ¯æ„å»ºæˆä¸€ä¸ªçŸ¥è¯†åº“ï¼ˆä»¥å¤šç§æ–‡ä»¶å½¢å¼å­˜å‚¨åœ¨ `working_dir` ä¸­ï¼‰ã€‚
2.  **æ¨ç†/é—®ç­” (Inference Phase)**: ç„¶åï¼Œå®ƒä¼šåŠ è½½ä¸€ä¸ªåŒ…å«é—®é¢˜çš„æ•°æ®é›†ï¼ˆ`dataset.json`ï¼‰ï¼Œå¹¶åˆ©ç”¨ç¬¬ä¸€é˜¶æ®µæ„å»ºçš„çŸ¥è¯†åº“ï¼Œå¯¹è¿™äº›é—®é¢˜è¿›è¡Œå›ç­”ï¼Œå¹¶å°†ç­”æ¡ˆä¿å­˜ä¸ºå•ç‹¬çš„ Markdown æ–‡ä»¶ã€‚

ä»¥ä¸‹æ˜¯è„šæœ¬æ‰§è¡Œçš„è¯¦ç»†æ­¥éª¤åˆ†è§£ã€‚

---

## ç¬¬ä¸€é˜¶æ®µï¼šå­¦ä¹ /ç´¢å¼• (Learn Phase)

æ­¤é˜¶æ®µç”± `videorag.insert_video(video_path_list=video_paths)` è¿™ä¸€è¡Œä»£ç è§¦å‘ã€‚`VideoRAG` ç±»çš„ `insert_video` æ–¹æ³•ä¼šéå†æ‰€æœ‰è¾“å…¥çš„è§†é¢‘æ–‡ä»¶ï¼Œå¹¶å¯¹æ¯ä¸€ä¸ªè§†é¢‘æ‰§è¡Œä»¥ä¸‹ä¸€ç³»åˆ—æ“ä½œã€‚è¿™äº›æ“ä½œå®šä¹‰åœ¨ `videorag/_videoutil/` ç›®å½•ä¸‹çš„å„ä¸ªæ¨¡å—ä¸­ã€‚

1.  **è§†é¢‘åˆ†å‰² (`split.py`)**:
    *   **ç›®æ ‡**: å°†é•¿è§†é¢‘åˆ†å‰²æˆä¸€ç³»åˆ—å›ºå®šé•¿åº¦çš„ã€æ›´æ˜“äºå¤„ç†çš„å°è§†é¢‘ç‰‡æ®µã€‚
    *   **è¿‡ç¨‹**:
        *   è„šæœ¬è°ƒç”¨ `split_video` å‡½æ•°ã€‚
        *   è¯¥å‡½æ•°ä½¿ç”¨ `moviepy` åº“æ¥è¯»å–è§†é¢‘ï¼Œå¹¶æ ¹æ® `video_segment_length` å‚æ•°ï¼ˆé»˜è®¤ä¸º 30 ç§’ï¼‰å°†è§†é¢‘åˆ‡å‰²æˆå¤šä¸ªç‰‡æ®µã€‚
        *   æ¯ä¸ªç‰‡æ®µéƒ½ä¼šè¢«ä¿å­˜ä¸ºä¸€ä¸ªç‹¬ç«‹çš„è§†é¢‘æ–‡ä»¶ï¼Œå¹¶åŒæ—¶æå–å‡ºå…¶å¯¹åº”çš„éŸ³é¢‘ï¼Œä¿å­˜ä¸ºéŸ³é¢‘æ–‡ä»¶ï¼ˆé»˜è®¤ä¸º mp3ï¼‰ã€‚
        *   æ­¤å¤–ï¼Œå®ƒè¿˜ä¼šä¸ºæ¯ä¸ªç‰‡æ®µé‡‡æ ·ä¸€å®šæ•°é‡çš„å¸§ï¼ˆç”± `rough_num_frames_per_segment` å®šä¹‰ï¼‰ï¼Œè¿™äº›å¸§çš„è·¯å¾„æˆ–æ—¶é—´æˆ³ä¿¡æ¯ä¼šè¢«è®°å½•ä¸‹æ¥ï¼Œç”¨äºåç»­çš„è§†è§‰åˆ†æã€‚

2.  **è¯­éŸ³è¯†åˆ« (ASR - `asr.py`)**:
    *   **ç›®æ ‡**: å°†æ¯ä¸ªè§†é¢‘ç‰‡æ®µçš„éŸ³é¢‘è½¬æ¢æˆæ–‡å­—ç¨¿ã€‚
    *   **è¿‡ç¨‹**:
        *   è„šæœ¬è°ƒç”¨ `speech_to_text` å‡½æ•°ã€‚
        *   åœ¨ `VideoRAG-algorithm` é¡¹ç›®ä¸­ï¼Œè¿™é€šå¸¸ä¼šè°ƒç”¨ä¸€ä¸ªæœ¬åœ°æ¨¡å‹ï¼Œä¾‹å¦‚ `faster_whisper`ã€‚å®ƒä¼šåŠ è½½ä¸€ä¸ªé¢„è®­ç»ƒçš„ Whisper æ¨¡å‹ã€‚
        *   å¯¹äºæ¯ä¸ªéŸ³é¢‘æ–‡ä»¶ï¼Œæ¨¡å‹ä¼šè¿›è¡Œæ¨ç†ï¼Œç”Ÿæˆå¯¹åº”çš„æ–‡å­—è®°å½•ï¼ˆtranscriptï¼‰ã€‚
        *   è¿™äº›æ–‡å­—è®°å½•ä¸å®ƒä»¬æ‰€å±çš„è§†é¢‘ç‰‡æ®µä¸€ä¸€å¯¹åº”ã€‚

3.  **è§†è§‰åˆ†æ/è§†é¢‘å­—å¹• (`caption.py`)**:
    *   **ç›®æ ‡**: åˆ†ææ¯ä¸ªè§†é¢‘ç‰‡æ®µçš„è§†è§‰å†…å®¹ï¼Œç”Ÿæˆä¸€æ®µæè¿°æ€§çš„æ–‡å­—ï¼ˆå³å­—å¹•æˆ– captionï¼‰ã€‚
    *   **è¿‡ç¨‹**:
        *   è„šæœ¬è°ƒç”¨ `segment_caption` å‡½æ•°ã€‚
        *   è¯¥å‡½æ•°ä¼šåˆ©ç”¨ä¸€ä¸ªå¤šæ¨¡æ€çš„è§†è§‰è¯­è¨€æ¨¡å‹ (VLM)ã€‚
        *   å®ƒå°†ä¹‹å‰é‡‡æ ·å¥½çš„è§†é¢‘å¸§å’Œå¯¹åº”çš„ ASR æ–‡å­—ç¨¿ä¸€èµ·ä½œä¸ºè¾“å…¥ï¼Œæä¾›ç»™ VLMã€‚
        *   VLM ä¼šâ€œè§‚çœ‹â€è¿™äº›å¸§å¹¶â€œé˜…è¯»â€æ–‡å­—ç¨¿ï¼Œç„¶åç”Ÿæˆä¸€æ®µæ€»ç»“æ€§çš„ã€æè¿°è¯¥è§†é¢‘ç‰‡æ®µå†…å®¹çš„è‹±æ–‡æ–‡å­—ã€‚

4.  **ä¿¡æ¯æ•´åˆ**:
    *   **ç›®æ ‡**: å°†æ¥è‡ªä¸åŒæ¨¡å‹çš„ä¿¡æ¯ï¼ˆéŸ³é¢‘æ–‡å­—ã€è§†é¢‘å­—å¹•ã€æ—¶é—´æˆ³ï¼‰åˆå¹¶åœ¨ä¸€èµ·ã€‚
    *   **è¿‡ç¨‹**:
        *   `merge_segment_information` å‡½æ•°è¢«è°ƒç”¨ã€‚
        *   å®ƒå°†æ¯ä¸ªè§†é¢‘ç‰‡æ®µçš„æ—¶é—´æˆ³ã€ASR æ–‡å­—ç¨¿å’Œè§†è§‰å­—å¹•ç»„åˆæˆä¸€ä¸ªç»“æ„åŒ–çš„æ•°æ®å•å…ƒã€‚è¿™åˆ›å»ºäº†å¯¹æ¯ä¸ªè§†é¢‘ç‰‡æ®µçš„å¤šæ¨¡æ€æè¿°ã€‚

5.  **ç‰¹å¾ç¼–ç  (`feature.py`)**:
    *   **ç›®æ ‡**: å°†è§†é¢‘ç‰‡æ®µçš„è§†è§‰å†…å®¹è½¬æ¢æˆä¸€ä¸ªé«˜ç»´çš„æ•°å­¦å‘é‡ï¼ˆembeddingï¼‰ï¼Œä»¥ä¾¿è¿›è¡Œå¿«é€Ÿçš„ç›¸ä¼¼æ€§æœç´¢ã€‚
    *   **è¿‡ç¨‹**:
        *   è„šæœ¬è°ƒç”¨ `video_segment_feature_vdb.upsert` æ–¹æ³•ï¼Œå†…éƒ¨ä¼šä½¿ç”¨ `ImageBind` æˆ–ç±»ä¼¼çš„æ¨¡å‹ã€‚
        *   `ImageBind` æ˜¯ä¸€ä¸ªèƒ½å°†å¤šç§æ¨¡æ€ï¼ˆè§†é¢‘ã€å›¾åƒã€æ–‡æœ¬ã€éŸ³é¢‘ç­‰ï¼‰ç¼–ç åˆ°åŒä¸€ä¸ªå‘é‡ç©ºé—´çš„å¤šæ¨¡æ€ç¼–ç å™¨ã€‚
        *   å¯¹äºæ¯ä¸ªè§†é¢‘ç‰‡æ®µï¼Œæ¨¡å‹ä¼šå¤„ç†å…¶è§†è§‰å†…å®¹ï¼Œå¹¶è¾“å‡ºä¸€ä¸ªå›ºå®šç»´åº¦çš„ç‰¹å¾å‘é‡ã€‚
        *   è¿™äº›å‘é‡è¢«å­˜å‚¨åœ¨ä¸€ä¸ªå‘é‡æ•°æ®åº“ä¸­ï¼ˆä¾‹å¦‚ `HNSWlib` æˆ– `NanoVectorDB`ï¼‰ï¼Œå¹¶ä¸å®ƒä»¬æ‰€ä»£è¡¨çš„è§†é¢‘ç‰‡æ®µçš„ ID å…³è”èµ·æ¥ã€‚

6.  **çŸ¥è¯†å›¾è°±æ„å»º (`_op.py`)**:
    *   **ç›®æ ‡**: ä»æ•´åˆåçš„æ–‡æœ¬ä¿¡æ¯ï¼ˆASR + Captionï¼‰ä¸­æå–å…³é”®å®ä½“ï¼ˆå¦‚äººåã€åœ°åã€æ¦‚å¿µï¼‰ï¼Œå¹¶æ„å»ºå®ƒä»¬ä¹‹é—´çš„å…³ç³»ï¼Œå½¢æˆä¸€ä¸ªçŸ¥è¯†å›¾è°±ã€‚
    *   **è¿‡ç¨‹**:
        *   åœ¨ `insert_video` çš„æœ€åï¼Œè„šæœ¬ä¼šè°ƒç”¨ `self.ainsert` æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å†…éƒ¨ä¼šè§¦å‘ `extract_entities` å‡½æ•°ã€‚
        *   è¿™ä¸ªå‡½æ•°ä½¿ç”¨ä¸€ä¸ªå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼Œä¾‹å¦‚ GPT-4o-miniï¼‰æ¥è¯»å–æ¯ä¸ªç‰‡æ®µçš„æ–‡æœ¬ã€‚
        *   LLM è¢«æŒ‡ç¤ºå»è¯†åˆ«æ–‡æœ¬ä¸­çš„å‘½åå®ä½“ï¼Œå¹¶å°†å®ƒä»¬çš„å…³ç³»ä»¥å›¾ï¼ˆGraphï¼‰çš„å½¢å¼è¿›è¡Œç»„ç»‡ã€‚
        *   è¿™ä¸ªå›¾è¢«å­˜å‚¨ä¸‹æ¥ï¼ˆä¾‹å¦‚ï¼Œä½¿ç”¨ `NetworkX` åº“ï¼‰ï¼Œå®ƒæ•æ‰äº†è§†é¢‘å†…å®¹çš„æ ¸å¿ƒè¯­ä¹‰å’ŒçŸ¥è¯†ç»“æ„ã€‚

å®Œæˆä»¥ä¸Šæ‰€æœ‰æ­¥éª¤åï¼Œ`working_dir` ç›®å½•ä¸­ä¼šåŒ…å«è§†é¢‘çš„æ–‡æœ¬ä¿¡æ¯ã€ç‰¹å¾å‘é‡ã€çŸ¥è¯†å›¾è°±ç­‰ä¸€ç³»åˆ—æ–‡ä»¶ã€‚è¿™äº›æ–‡ä»¶å…±åŒæ„æˆäº†å¯¹åŸå§‹è§†é¢‘é›†åˆçš„å®Œæ•´çŸ¥è¯†ç´¢å¼•ï¼Œä¸ºç¬¬äºŒé˜¶æ®µçš„é—®ç­”åšå¥½äº†å‡†å¤‡ã€‚


---

## ç¬¬äºŒé˜¶æ®µï¼šæ¨ç†/é—®ç­” (Inference Phase)

åœ¨å®Œæˆè§†é¢‘ç´¢å¼•åï¼Œè„šæœ¬ä¼šç«‹å³è¿›å…¥é—®ç­”é˜¶æ®µã€‚

1.  **åŠ è½½é—®é¢˜é›†**:
    *   è„šæœ¬é¦–å…ˆä¼šè¯»å– `longervideos/dataset.json` æ–‡ä»¶ã€‚è¿™ä¸ª JSON æ–‡ä»¶åŒ…å«äº†ä¸€ç³»åˆ—çš„é—®é¢˜ï¼Œæ¯ä¸ªé—®é¢˜éƒ½æœ‰ä¸€ä¸ªå”¯ä¸€çš„ ID å’Œå…·ä½“çš„é—®é¢˜æ–‡æœ¬ã€‚

2.  **é‡æ–°åˆå§‹åŒ– `VideoRAG` å®ä¾‹**:
    *   è„šæœ¬ä¼šå†æ¬¡åˆ›å»ºä¸€ä¸ª `VideoRAG` ç±»çš„å®ä¾‹ã€‚é‡è¦çš„æ˜¯ï¼Œå®ƒä¼ å…¥äº†ä¸ç¬¬ä¸€é˜¶æ®µ**å®Œå…¨ç›¸åŒ**çš„ `working_dir`ã€‚è¿™ä½¿å¾—æ–°å®ä¾‹èƒ½å¤Ÿè‡ªåŠ¨åŠ è½½ä¹‹å‰å·²ç»æ„å»ºå¥½çš„æ‰€æœ‰ç´¢å¼•æ–‡ä»¶ï¼ˆçŸ¥è¯†å›¾è°±ã€å‘é‡æ•°æ®åº“ç­‰ï¼‰ã€‚

3.  **åŠ è½½å­—å¹•æ¨¡å‹**:
    *   è„šæœ¬æ˜¾å¼è°ƒç”¨ `videorag.load_caption_model()`ã€‚è¿™æ˜¯ä¸€ä¸ªåœ¨ `Vimo-desktop` ç‰ˆæœ¬ä¸­ä¸å­˜åœ¨çš„æ–¹æ³•ï¼Œå®ƒå¯èƒ½æ˜¯ç”¨æ¥é¢„åŠ è½½åœ¨é—®ç­”é˜¶æ®µéœ€è¦ç”¨åˆ°çš„å¤šæ¨¡æ€å¤§æ¨¡å‹ï¼Œä»¥æé«˜åç»­å¤„ç†çš„æ•ˆç‡ã€‚

4.  **å¾ªç¯å›ç­”é—®é¢˜**:
    *   è„šæœ¬ä¼šéå†ä» `dataset.json` ä¸­åŠ è½½çš„æ‰€æœ‰é—®é¢˜ã€‚å¯¹äºæ¯ä¸€ä¸ªé—®é¢˜ï¼Œå®ƒä¼šæ‰§è¡Œä»¥ä¸‹æ“ä½œï¼š
        *   **è°ƒç”¨æŸ¥è¯¢æ–¹æ³•**: `videorag.query(query=query, param=param)` è¢«è°ƒç”¨ã€‚è¿™æ˜¯æ‰§è¡Œ RAG (Retrieval-Augmented Generationï¼Œæ£€ç´¢å¢å¼ºç”Ÿæˆ) çš„æ ¸å¿ƒã€‚
        *   **æ£€ç´¢ (Retrieval)**:
            *   `query` æ–¹æ³•é¦–å…ˆä¼šä½¿ç”¨æ–‡æœ¬ç¼–ç æ¨¡å‹ï¼ˆå¦‚ `text-embedding-3-small`ï¼‰å°†ç”¨æˆ·çš„é—®é¢˜è½¬æ¢æˆä¸€ä¸ªå‘é‡ã€‚
            *   ç„¶åï¼Œå®ƒæ‹¿ç€è¿™ä¸ªæŸ¥è¯¢å‘é‡ï¼Œåˆ°ç¬¬ä¸€é˜¶æ®µæ„å»ºçš„å‘é‡æ•°æ®åº“ä¸­è¿›è¡Œç›¸ä¼¼æ€§æœç´¢ï¼Œæ‰¾å‡ºä¸é—®é¢˜æœ€ç›¸å…³çš„è§†é¢‘ç‰‡æ®µçš„ç‰¹å¾å‘é‡ã€‚
            *   åŒæ—¶ï¼Œå®ƒå¯èƒ½è¿˜ä¼šåˆ©ç”¨çŸ¥è¯†å›¾è°±æ¥æŸ¥æ‰¾ä¸é—®é¢˜ä¸­çš„å®ä½“ç›¸å…³çš„å…¶ä»–ä¿¡æ¯ã€‚
        *   **å¢å¼º (Augmentation)**:
            *   è„šæœ¬å°†æ£€ç´¢åˆ°çš„æœ€ç›¸å…³çš„è§†é¢‘ç‰‡æ®µçš„**åŸå§‹ä¿¡æ¯**ï¼ˆåŒ…æ‹¬ ASR æ–‡å­—ç¨¿ã€è§†é¢‘å­—å¹•ã€æ—¶é—´æˆ³ç­‰ï¼‰æå–å‡ºæ¥ã€‚
            *   è¿™äº›ä¿¡æ¯è¢«ç»„åˆæˆä¸€ä¸ªä¸°å¯Œçš„ä¸Šä¸‹æ–‡ï¼ˆcontextï¼‰ã€‚
        *   **ç”Ÿæˆ (Generation)**:
            *   æœ€åï¼Œè„šæœ¬å°†åŸå§‹çš„ç”¨æˆ·é—®é¢˜å’Œåˆšåˆšæ„å»ºçš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œä¸€èµ·å‘é€ç»™ä¸€ä¸ªå¼ºå¤§çš„å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼Œå¦‚ GPT-4o-miniï¼‰ã€‚
            *   å®ƒä¼šè¦æ±‚ LLM åœ¨æä¾›çš„ä¸Šä¸‹æ–‡ä¿¡æ¯çš„åŸºç¡€ä¸Šï¼Œæ¥å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚è¿™ç§â€œå…ˆæ£€ç´¢ã€åç”Ÿæˆâ€çš„æ–¹å¼ï¼Œä½¿å¾— LLM èƒ½å¤Ÿå›ç­”å…³äºè§†é¢‘å†…å®¹çš„éå¸¸å…·ä½“å’Œæ·±å…¥çš„é—®é¢˜ï¼Œè€Œä¸æ˜¯ä»…ä»…ä¾èµ–å…¶å†…éƒ¨çŸ¥è¯†ã€‚
        *   **ä¿å­˜ç­”æ¡ˆ**: LLM ç”Ÿæˆçš„æœ€ç»ˆç­”æ¡ˆä¼šè¢«ä¿å­˜åˆ°ä¸€ä¸ªä½äº `longervideos/videorag-answers/` ç›®å½•ä¸‹çš„ Markdown æ–‡ä»¶ä¸­ï¼Œæ–‡ä»¶åä¸é—®é¢˜çš„ ID å¯¹åº”ã€‚

è¿™ä¸ªæµç¨‹å®Œæ•´åœ°å±•ç¤ºäº† RAG æ¡†æ¶å¦‚ä½•è¢«åº”ç”¨äºè§†é¢‘ç†è§£ä»»åŠ¡ä¸­ï¼šå…ˆå°†è§†é¢‘å†…å®¹â€œçŸ¥è¯†åŒ–â€ï¼ˆç´¢å¼•é˜¶æ®µï¼‰ï¼Œç„¶ååœ¨å›ç­”é—®é¢˜æ—¶ï¼Œç²¾ç¡®åœ°æ£€ç´¢å‡ºç›¸å…³çš„çŸ¥è¯†ç‰‡æ®µï¼Œå¹¶è®©å¤§è¯­è¨€æ¨¡å‹åŸºäºè¿™äº›å…·ä½“çš„çŸ¥è¯†æ¥ç”Ÿæˆç­”æ¡ˆã€‚


---

## ç¬¬ä¸‰éƒ¨åˆ†ï¼šä¸­æ–‡æ¨¡å‹æ›¿æ¢æŒ‡å—

æœ¬æŒ‡å—æ—¨åœ¨ä¸ºæ‚¨æä¾›åœ¨ `VideoRAG-algorithm` é¡¹ç›®ä¸­æ›¿æ¢å’Œé›†æˆæ›´é€‚åˆä¸­æ–‡è§†é¢‘å¤„ç†çš„æœ¬åœ°åŒ–æ¨¡å‹çš„å…·ä½“æ­¥éª¤å’Œå»ºè®®ã€‚

### æ€»ä½“æ€è·¯

ä»£ç ä¸­ä¸æ¨¡å‹ç›¸å…³çš„æ ¸å¿ƒé…ç½®ä½äº `videorag_longervideos.py` è„šæœ¬é¡¶éƒ¨çš„ `longervideos_llm_config` å¯¹è±¡ä¸­ã€‚ç„¶è€Œï¼Œè¿™ä¸ªé…ç½®å¯¹è±¡æ‰€å¼•ç”¨çš„å…·ä½“æ¨¡å‹è°ƒç”¨å‡½æ•°åˆ™å®šä¹‰åœ¨ `videorag/_llm.py` æ–‡ä»¶é‡Œã€‚

å› æ­¤ï¼Œæ›¿æ¢æ¨¡å‹é€šå¸¸éœ€è¦ä¸¤æ­¥ï¼š

1.  **ä¿®æ”¹ `videorag/_llm.py`**: åœ¨æ­¤æ–‡ä»¶ä¸­æ·»åŠ æ–°çš„å‡½æ•°ï¼Œç”¨äºè°ƒç”¨æ‚¨åœ¨æœ¬åœ°éƒ¨ç½²çš„æ–°æ¨¡å‹ï¼ˆä¾‹å¦‚ï¼Œé€šè¿‡ API è¯·æ±‚æœ¬åœ°çš„æ¨ç†æœåŠ¡å™¨ï¼‰ã€‚
2.  **ä¿®æ”¹ `videorag_longervideos.py`**: æ›´æ–° `longervideos_llm_config` å¯¹è±¡ï¼Œä½¿å…¶å¼•ç”¨æ‚¨åœ¨ `_llm.py` ä¸­æ–°åˆ›å»ºçš„å‡½æ•°ï¼Œå¹¶ä¼ å…¥æ–°æ¨¡å‹çš„åç§°ã€‚

ä»¥ä¸‹æ˜¯é’ˆå¯¹è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰å’Œè§†é¢‘å­—å¹•ï¼ˆCaptionï¼‰æ¨¡å‹çš„å…·ä½“æ›¿æ¢å»ºè®®ã€‚


### 1. è¯­éŸ³è¯†åˆ« (ASR) æ¨¡å‹æ›¿æ¢

**æ¨èæ¨¡å‹**: **FunASR Paraformer-large**

*   **ä¼˜ç‚¹**: è¿™æ˜¯ç”±é˜¿é‡Œå·´å·´è¾¾æ‘©é™¢å¼€å‘çš„ä¸šç•Œé¢†å…ˆçš„ä¸­æ–‡è¯­éŸ³è¯†åˆ«æ¨¡å‹ã€‚å®ƒåœ¨å‡†ç¡®ç‡ä¸Šè¡¨ç°éå¸¸å‡ºè‰²ï¼Œå°¤å…¶æ“…é•¿å¤„ç†å¸¦æœ‰å£éŸ³ã€è¯­é€Ÿå˜åŒ–å’ŒèƒŒæ™¯å™ªéŸ³çš„çœŸå®åœºæ™¯è¯­éŸ³ã€‚å®ƒå®Œå…¨å¼€æºï¼Œå¹¶ä¸”æœ‰æˆç†Ÿçš„æœ¬åœ°éƒ¨ç½²æ–¹æ¡ˆã€‚
*   **éƒ¨ç½²**: æ‚¨éœ€è¦æ ¹æ® [FunASR çš„å®˜æ–¹æ–‡æ¡£](https://github.com/alibaba-damo-academy/FunASR) åœ¨æ‚¨çš„æœ¬åœ°ï¼ˆæˆ– WSLï¼‰ç¯å¢ƒä¸­éƒ¨ç½²å…¶æ¨ç†æœåŠ¡ã€‚é€šå¸¸ï¼Œè¿™ä¼šæ¶‰åŠåˆ°è¿è¡Œä¸€ä¸ª Docker å®¹å™¨æˆ–ä¸€ä¸ª Python æœåŠ¡å™¨ï¼Œå®ƒä¼šåœ¨æœ¬åœ°æš´éœ²ä¸€ä¸ª API ç«¯ç‚¹ï¼ˆä¾‹å¦‚ `http://localhost:8000/asr`ï¼‰ã€‚

**ä¿®æ”¹æ­¥éª¤**:

**a) ä¿®æ”¹ `videorag/_videoutil/asr.py`**

å½“å‰ï¼Œ`asr.py` ä¸­çš„ `speech_to_text_online` å‡½æ•°æ˜¯ä¸ºè°ƒç”¨é˜¿é‡Œäº‘ DashScope çš„åœ¨çº¿ API è®¾è®¡çš„ã€‚æˆ‘ä»¬éœ€è¦åˆ›å»ºä¸€ä¸ªæ–°çš„å‡½æ•°ï¼Œæˆ–è€…ä¿®æ”¹ç°æœ‰çš„å‡½æ•°ï¼Œæ¥è°ƒç”¨æ‚¨æœ¬åœ°çš„ FunASR æœåŠ¡ã€‚

**ç¤ºä¾‹ - æ·»åŠ ä¸€ä¸ªæ–°çš„æœ¬åœ° ASR å‡½æ•°**:

åœ¨ `asr.py` æ–‡ä»¶ä¸­ï¼Œæ‚¨å¯ä»¥æ·»åŠ å¦‚ä¸‹å‡½æ•°ï¼š

```python
# a_sr.py

import requests
import json

# ... (ä¿ç•™æ–‡ä»¶ä¸­çš„å…¶ä»– import)

# æ–°å¢å‡½æ•°ï¼Œç”¨äºè°ƒç”¨æœ¬åœ° FunASR æœåŠ¡
def call_funasr_local(audio_file_path: str) -> str:
    """
    Calls the local FunASR server to transcribe an audio file.
    """
    # FunASR é€šå¸¸éœ€è¦æ‚¨å°†æ–‡ä»¶ä»¥äºŒè¿›åˆ¶å½¢å¼ä¸Šä¼ 
    try:
        with open(audio_file_path, "rb") as f:
            files = {"audio_file": (os.path.basename(audio_file_path), f, "audio/mpeg")}
            # è¿™é‡Œçš„ URL "http://localhost:8000/asr" éœ€è¦æ ¹æ®æ‚¨è‡ªå·±çš„ FunASR éƒ¨ç½²åœ°å€è¿›è¡Œä¿®æ”¹
            response = requests.post("http://localhost:8000/asr", files=files)
            response.raise_for_status()  # å¦‚æœè¯·æ±‚å¤±è´¥åˆ™æŠ›å‡ºå¼‚å¸¸

            # è§£æ FunASR çš„è¿”å›ç»“æœï¼Œè¿™åŒæ ·éœ€è¦æ ¹æ® FunASR çš„ API æ–‡æ¡£æ¥ç¡®å®š
            # å‡è®¾å®ƒè¿”å›ä¸€ä¸ª JSONï¼Œå…¶ä¸­åŒ…å«ä¸€ä¸ª "text" å­—æ®µ
            result_json = response.json()
            return result_json.get("text", "")

    except requests.exceptions.RequestException as e:
        logger.error(f"FunASR request failed for {audio_file_path}: {e}")
        return ""
    except Exception as e:
        logger.error(f"Failed to process audio file {audio_file_path} with FunASR: {e}")
        return ""

# æ‚¨å¯ä»¥åˆ›å»ºä¸€ä¸ªæ–°çš„ä¸»è°ƒç”¨å‡½æ•°ï¼Œæˆ–è€…ä¿®æ”¹ç°æœ‰çš„ `speech_to_text_online`
async def speech_to_text_local_funasr(video_name, working_dir, segment_index2name, audio_output_format, global_config, max_concurrent=5):
    cache_path = os.path.join(working_dir, '_cache', video_name)
    transcripts = {}

    logger.info(f"ğŸ¤ Starting LOCAL ASR for {len(segment_index2name)} audio segments...")

    # è¿™é‡Œå¯ä»¥ä½¿ç”¨å¤šçº¿ç¨‹æˆ–å¼‚æ­¥æ¥å¹¶è¡Œå¤„ç†
    for index, segment_name in tqdm(segment_index2name.items(), desc="Transcribing Audio"):
        audio_file = os.path.join(cache_path, f"{segment_name}.{audio_output_format}")
        transcripts[index] = call_funasr_local(audio_file)

    logger.info("ğŸ‰ Local ASR processing completed!")
    return transcripts

# æœ€åï¼Œä¿®æ”¹ `speech_to_text` è¿™ä¸ªä¸»å…¥å£å‡½æ•°
def speech_to_text(video_name, working_dir, segment_index2name, audio_output_format, global_config):
    """
    Synchronous wrapper for speech-to-text function.
    Chooses between online and local based on config.
    """
    # æˆ‘ä»¬å¯ä»¥é€šè¿‡æ¨¡å‹åç§°æ¥åˆ¤æ–­æ˜¯ä½¿ç”¨åœ¨çº¿æœåŠ¡è¿˜æ˜¯æœ¬åœ°æœåŠ¡
    asr_model_name = global_config.get("asr_model", "")

    if "funasr" in asr_model_name.lower():
        # å¦‚æœæ¨¡å‹åç§°åŒ…å« "funasr"ï¼Œåˆ™è°ƒç”¨æœ¬åœ°æœåŠ¡
        loop = asyncio.get_event_loop()
        return loop.run_until_complete(
            speech_to_text_local_funasr(video_name, working_dir, segment_index2name, audio_output_format, global_config)
        )
    else:
        # å¦åˆ™ï¼Œä¿æŒåŸæœ‰çš„åœ¨çº¿æœåŠ¡é€»è¾‘
        loop = asyncio.get_event_loop()
        return loop.run_until_complete(
            speech_to_text_async(video_name, working_dir, segment_index2name, audio_output_format, global_config)
        )

```

**b) ä¿®æ”¹ `videorag_longervideos.py`**

ç°åœ¨ï¼Œæ‚¨åªéœ€è¦åœ¨ä¸»è„šæœ¬ä¸­æ›´æ–° `asr_model` çš„åç§°å³å¯ã€‚

```python
# videorag_longervideos.py

# ... (å…¶ä»–ä»£ç )

videorag = VideoRAG(
    llm=longervideos_llm_config,
    working_dir=f"./longervideos/videorag-workdir/{sub_category}",
    # åœ¨è¿™é‡Œæˆ–è€…åœ¨ LLMConfig ä¸­ï¼Œç¡®ä¿ asr_model è¢«è®¾ç½®
    asr_model="funasr-paraformer-large"  # ä½¿ç”¨ä¸€ä¸ªåŒ…å« "funasr" çš„æ–°åç§°
)
videorag.insert_video(video_path_list=video_paths)

# ... (å…¶ä»–ä»£ç )
```

é€šè¿‡ä»¥ä¸Šä¿®æ”¹ï¼Œå½“æ‚¨è¿è¡Œ `videorag_longervideos.py` æ—¶ï¼Œ`speech_to_text` å‡½æ•°ä¼šæ£€æµ‹åˆ°æ¨¡å‹åç§°ä¸­å«æœ‰ "funasr"ï¼Œå¹¶è‡ªåŠ¨åˆ‡æ¢åˆ°è°ƒç”¨æ‚¨æœ¬åœ°éƒ¨ç½²çš„ FunASR æœåŠ¡ã€‚


### 2. è§†é¢‘å­—å¹• (Caption) æ¨¡å‹æ›¿æ¢

**é›†æˆæ¨¡å‹**: **MiniCPM-V**

*   **ä¼˜ç‚¹**: è¿™æ˜¯ä¸€ä¸ªå¼ºå¤§çš„å¼€æºå¤šæ¨¡æ€å¤§æ¨¡å‹ï¼Œèƒ½å¤Ÿå¤„ç†è§†è§‰å’Œè¯­è¨€ä»»åŠ¡ã€‚å®ƒéå¸¸é€‚åˆä¸ºè§†é¢‘ç‰‡æ®µç”Ÿæˆæè¿°æ€§å­—å¹•ã€‚
*   **éƒ¨ç½²**: æ‚¨éœ€è¦æ ¹æ® MiniCPM-V çš„å®˜æ–¹æ–‡æ¡£ï¼Œåœ¨æœ¬åœ°éƒ¨ç½²å…¶æ¨ç†æœåŠ¡ã€‚é€šå¸¸ï¼Œè¿™ä¼šé€šè¿‡ `vLLM` æˆ–ç±»ä¼¼çš„æ¡†æ¶æ¥å®Œæˆï¼Œæœ€ç»ˆä¼šåœ¨æœ¬åœ°æš´éœ²ä¸€ä¸ªä¸ OpenAI API å…¼å®¹çš„ API ç«¯ç‚¹ï¼ˆä¾‹å¦‚ `http://localhost:8001/v1`ï¼‰ã€‚

**ä¿®æ”¹æ­¥éª¤**:

**a) ä¿®æ”¹ `videorag/_llm.py`**

æˆ‘ä»¬éœ€è¦åœ¨æ­¤æ–‡ä»¶ä¸­æ·»åŠ ä¸€ä¸ªæ–°å‡½æ•°ï¼Œç”¨äºä¸æœ¬åœ°éƒ¨ç½²çš„ MiniCPM-V æ¨¡å‹è¿›è¡Œäº¤äº’ã€‚æˆ‘ä»¬è¿˜éœ€è¦ç¡®ä¿ `LLMConfig` æ•°æ®ç±»å¯ä»¥ä¿å­˜å­—å¹•æ¨¡å‹çš„ä¿¡æ¯ã€‚

**ç¤ºä¾‹ - æ·»åŠ ä¸€ä¸ªæ–°çš„æœ¬åœ° Caption å‡½æ•°**:

åœ¨ `_llm.py` æ–‡ä»¶ä¸­ï¼Œæ·»åŠ ä»¥ä¸‹å‡½æ•°ã€‚æ­¤å‡½æ•°å°†è¿æ¥åˆ°æœ¬åœ°æ¨¡å‹æœåŠ¡å™¨ï¼Œå‘é€è§†é¢‘å¸§å’Œæ–‡æœ¬æç¤ºï¼Œå¹¶è¿”å›ç”Ÿæˆçš„å­—å¹•ã€‚

```python
# _llm.py

import base64
from io import BytesIO
from PIL import Image
from openai import AsyncOpenAI
from logging import getLogger

logger = getLogger(__name__)

# ... (ä¿ç•™æ–‡ä»¶ä¸­çš„å…¶ä»– import å’Œå‡½æ•°)

async def minicpm_v_caption_complete(
    model_name: str, content_list: list, **kwargs
) -> str:
    """
    è°ƒç”¨æœ¬åœ°çš„ã€ä¸ OpenAI API å…¼å®¹çš„ MiniCPM-V æ¨¡å‹ç«¯ç‚¹ã€‚
    """
    global_config = kwargs.get("global_config", {})

    local_api_base = global_config.get("local_vlm_base_url", "http://localhost:8001/v1")

    local_client = AsyncOpenAI(
        api_key="your-dummy-api-key",
        base_url=local_api_base,
    )

    processed_content = []
    for item in content_list:
        if item["type"] == "image_url":
            pil_image = item["image_url"]["url"]
            if isinstance(pil_image, Image.Image):
                buffered = BytesIO()
                pil_image.save(buffered, format="PNG")
                img_str = base64.b64encode(buffered.getvalue()).decode("utf-8")
                processed_content.append({
                    "type": "image_url",
                    "image_url": {"url": f"data:image/png;base64,{img_str}"}
                })
            else:
                 processed_content.append(item)
        else:
            processed_content.append(item)

    messages = [
        {"role": "system", "content": "You are a helpful assistant that describes video content in Chinese."},
        {"role": "user", "content": processed_content}
    ]

    try:
        response = await local_client.chat.completions.create(
            model=model_name,
            messages=messages,
        )
        return response.choices[0].message.content
    except Exception as e:
        logger.error(f"Local MiniCPM-V request failed: {e}")
        return ""

```

æ¥ä¸‹æ¥ï¼Œæ›´æ–° `LLMConfig` æ•°æ®ç±»ä»¥åŒ…å«å­—å¹•æ¨¡å‹é…ç½®ï¼š

```python
# _llm.py

@dataclass
class LLMConfig:
    # ... (ä¿ç•™æ‰€æœ‰ç°æœ‰å­—æ®µ)

    cheap_model_max_token_size: int
    cheap_model_max_async: int

    caption_model_func_raw: callable = None
    caption_model_name: str = None

    # Assigned in post init
    embedding_func: EmbeddingFunc  = None
    best_model_func: callable = None
    cheap_model_func: callable = None
```

**b) ä¿®æ”¹ `videorag/_videoutil/caption.py`**

ç°åœ¨ï¼Œæˆ‘ä»¬éœ€è¦æ›´æ–° `segment_caption` å‡½æ•°ï¼Œä½¿å…¶ä¸å†ç¡¬ç¼–ç æ¨¡å‹ï¼Œè€Œæ˜¯ä½¿ç”¨æˆ‘ä»¬é€šè¿‡ `LLMConfig` ä¼ å…¥çš„å‡½æ•°ã€‚

```python
# caption.py

import asyncio
from functools import partial
from moviepy.video.io.VideoFileClip import VideoFileClip
from tqdm import tqdm

# ... (ä¿ç•™ encode_video å‡½æ•°)

def segment_caption(video_name, video_path, segment_index2name, transcripts, segment_times_info, caption_result, error_queue, global_config=None):
    try:
        llm_config = global_config.get("llm", {})

        caption_model_func = llm_config.get("caption_model_func_raw")
        caption_model_name = llm_config.get("caption_model_name", "minicpm-v")

        if caption_model_func is None:
            raise ValueError("Caption model function not provided in LLMConfig.")

        caption_func = partial(caption_model_func, caption_model_name, global_config=global_config)

        async def run_captioning():
            with VideoFileClip(video_path) as video:
                for index in tqdm(segment_index2name, desc=f"Captioning Video {video_name}"):
                    frame_times = segment_times_info[index]["frame_times"]
                    video_frames = encode_video(video, frame_times)
                    segment_transcript = transcripts[index]

                    content_list = []
                    for frame in video_frames:
                        content_list.append({"type": "image_url", "image_url": {"url": frame}})
                    content_list.append({"type": "text", "text": f"The transcript of the current video:\n{segment_transcript}.\nNow provide a description (caption) of the video in Chinese."})

                    caption = await caption_func(content_list=content_list)
                    caption_result[index] = caption.replace("\n", "").replace("<|endoftext|>", "")

        asyncio.run(run_captioning())

    except Exception as e:
        error_queue.put(f"Error in segment_caption:\n {str(e)}")

```

**c) ä¿®æ”¹ `videorag/videorag.py`**

æˆ‘ä»¬éœ€è¦æ›´æ–° `insert_video` æ–¹æ³•ï¼Œä»¥å°†å…¨å±€é…ç½®ä¼ é€’ç»™ `segment_caption` è¿›ç¨‹ã€‚

```python
# videorag.py

# ... (åœ¨ insert_video æ–¹æ³•ä¸­)
            process_segment_caption = multiprocessing.Process(
                target=segment_caption,
                args=(
                    video_name,
                    video_path,
                    segment_index2name,
                    transcripts,
                    segment_times_info,
                    captions,
                    error_queue,
                    asdict(self), # ä¼ å…¥å…¨å±€é…ç½®
                )
            )
# ...
```

**d) ä¿®æ”¹ `videorag_longervideos.py`**

æœ€åï¼Œåœ¨ä¸»è„šæœ¬ä¸­ï¼Œæ›´æ–° `longervideos_llm_config` å¯¹è±¡ï¼Œä»¥ä½¿ç”¨æˆ‘ä»¬æ–°åˆ›å»ºçš„å‡½æ•°å’Œæ¨¡å‹ã€‚

```python
# videorag_longervideos.py

from videorag._llm import * # ç¡®ä¿æ–°å‡½æ•°è¢«å¯¼å…¥

# ...

longervideos_llm_config = LLMConfig(
    # ... (ä¿ç•™ embedding å’Œå…¶ä»–æ¨¡å‹çš„é…ç½®)

    # â†“â†“â†“ æ·»åŠ ä»¥ä¸‹éƒ¨åˆ† â†“â†“â†“
    # Caption model configuration
    caption_model_func_raw=minicpm_v_caption_complete,
    caption_model_name="minicpm-v" # æˆ–æ‚¨æœ¬åœ°æœåŠ¡å™¨ç‰¹å®šçš„æ¨¡å‹æ ‡è¯†ç¬¦
)

if __name__ == '__main__':
    # ... (åç»­ä»£ç ä¸å˜)
```

é€šè¿‡ä»¥ä¸Šä¿®æ”¹ï¼Œ`VideoRAG` å®ä¾‹åœ¨è¿›è¡Œè§†é¢‘å­—å¹•ç”Ÿæˆæ—¶ï¼Œå°†è°ƒç”¨ `minicpm_v_caption_complete` å‡½æ•°ï¼Œè¯¥å‡½æ•°ä¼šå°†è¯·æ±‚å‘é€åˆ°æ‚¨æœ¬åœ°éƒ¨ç½²çš„ MiniCPM-V æ¨¡å‹æœåŠ¡ï¼Œä»è€Œå®ç°äº†å®Œå…¨æœ¬åœ°åŒ–çš„ã€é«˜è´¨é‡çš„ä¸­æ–‡è§†é¢‘å†…å®¹åˆ†æã€‚
